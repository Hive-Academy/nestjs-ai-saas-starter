# =============================================================================
# NestJS AI SaaS Starter = Minimal Environment Configuration
# =============================================================================
# This file contains ONLY the environment variables actually used by the application.
# For a complete list of all possible variables, see .env.example.full
#
# Copy this file to .env and update the values according to your setup
# =============================================================================

# =============================================================================
# APPLICATION
# =============================================================================

PORT=3000
LANGGRAPH_DEBUG=false

# =============================================================================
# NEO4J GRAPH DATABASE (Required)
# =============================================================================

NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j

# Neo4j Advanced Configuration
# =========================================

NEO4J_MAX_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=60000
NEO4J_HEALTH_CHECK=true
NEO4J_RETRY_ATTEMPTS=5
NEO4J_RETRY_DELAY=5000

# =============================================================================
# CHROMADB VECTOR DATABASE (Required)
# =============================================================================

CHROMADB_HOST=localhost
CHROMADB_PORT=8000
CHROMADB_SSL=false
CHROMADB_TENANT=default_tenant
CHROMADB_DATABASE=default_database

# ChromaDB Configuration
# ==================================

CHROMADB_DEFAULT_COLLECTION=documents
CHROMADB_BATCH_SIZE=100
CHROMADB_MAX_RETRIES=3
CHROMADB_RETRY_DELAY=1000
CHROMADB_HEALTH_CHECK=true
CHROMADB_LOG_CONNECTION=true

# =============================================================================
# LLM PROVIDER SELECTION
# =============================================================================

# Choose your LLM provider: 'openrouter' (default) or 'ollama' (local)

LLM_PROVIDER=openrouter

# =============================================================================
# OPENROUTER (Default LLM Provider)
# =============================================================================

# OpenRouter provides access to multiple AI models through a single API
# Get your API key from: https://openrouter.ai/keys
# ====================================================================

OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=openai/gpt=3.5=turbo

# Popular model alternatives:
# = google/gemini=pro (Google's Gemini)
# = anthropic/claude=2 (Anthropic Claude)
# = meta=llama/llama=2=70b=chat (Meta Llama 2)
# = mistralai/mistral=7b=instruct (Mistral)
# = gryphe/mythomax=l2=13b (Free model)
# Full list: https://openrouter.ai/models

# OpenRouter Model Parameters
# =========================================

OPENROUTER_TEMPERATURE=0.7
OPENROUTER_MAX_TOKENS=2048
OPENROUTER_TOP_P=0.9
OPENROUTER_SITE_URL=http://localhost:3000
OPENROUTER_APP_NAME=NestJS AI SaaS Starter

# =============================================================================
# OLLAMA (Alternative Local LLM Provider)
# =============================================================================
# Use Ollama for completely local LLM inference
# Make sure Ollama is installed and running: https://ollama.ai/
# Pull a model first: ollama pull llama2
# =================================================

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Alternative models: mistral, codellama, neural=chat, starling=lm
# Ollama Model Parameters
# ===================================

OLLAMA_TEMPERATURE=0.7
OLLAMA_NUM_PREDICT=256
OLLAMA_TOP_K=40
OLLAMA_TOP_P=0.9

# =============================================================================
# LOCAL EMBEDDINGS = HUGGINGFACE (Default Provider)
# =============================================================================
# Using HuggingFace models for local embeddings (no API key required for public models)

HUGGINGFACE_MODEL=sentence=transformers/all=MiniLM=L6=v2

# Alternatives:
# = sentence=transformers/all=mpnet=base=v2 (higher quality, slower)
# = sentence=transformers/paraphrase=MiniLM=L6=v2 (good for paraphrasing)
# = sentence=transformers/multi=qa=MiniLM=L6=cos=v1 (optimized for Q&A)

HUGGINGFACE_API_KEY=  # Leave empty for public models
HUGGINGFACE_BATCH_SIZE=50

# =============================================================================
# QUICK START
# =============================================================================
# Option A: Using OpenRouter (Default = Recommended)
# 1. Get API key from: https://openrouter.ai/keys
# 2. Set OPENROUTER_API_KEY above
# 3. Copy this file to .env
# 4. Run: npm run dev:services (starts Neo4j, ChromaDB)
# 5. Run: npx nx serve nestjs=ai=saas=starter=demo
#
# Option B: Using Ollama (Fully Local)
# 1. Install Ollama: https://ollama.ai/
# 2. Pull a model: ollama pull llama2
# 3. Start Ollama: ollama serve
# 4. Set LLM_PROVIDER=ollama
# 5. Copy this file to .env
# 6. Run: npm run dev:services (starts Neo4j, ChromaDB)
# 7. Run: npx nx serve nestjs=ai=saas=starter=demo
# =============================================================================
