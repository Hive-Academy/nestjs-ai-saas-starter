# =============================================================================
# NestJS AI SaaS Starter = Minimal Environment Configuration
# =============================================================================
# This file contains ONLY the environment variables actually used by the application.
# For a complete list of all possible variables, see .env.example.full
#
# Copy this file to .env and update the values according to your setup
# =============================================================================

# =============================================================================
# APPLICATION
# =============================================================================

PORT=3000
LANGGRAPH_DEBUG=false

# =============================================================================
# NEO4J GRAPH DATABASE (Required)
# =============================================================================

NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j

# Neo4j Advanced Configuration
# =========================================

NEO4J_MAX_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=60000
NEO4J_HEALTH_CHECK=true
NEO4J_RETRY_ATTEMPTS=5
NEO4J_RETRY_DELAY=5000

# =============================================================================
# CHROMADB VECTOR DATABASE (Required)
# =============================================================================

CHROMADB_HOST=localhost
CHROMADB_PORT=8000
CHROMADB_SSL=false
CHROMADB_TENANT=default_tenant
CHROMADB_DATABASE=default_database

# ChromaDB Configuration
# ==================================

CHROMADB_DEFAULT_COLLECTION=documents
CHROMADB_BATCH_SIZE=100
CHROMADB_MAX_RETRIES=3
CHROMADB_RETRY_DELAY=1000
CHROMADB_HEALTH_CHECK=true
CHROMADB_LOG_CONNECTION=true

# =============================================================================
# LLM PROVIDER SELECTION
# =============================================================================

# Choose your LLM provider: 'openrouter' (default) or 'ollama' (local)

LLM_PROVIDER=openrouter

# =============================================================================
# OPENROUTER (Default LLM Provider)
# =============================================================================

# OpenRouter provides access to multiple AI models through a single API
# Get your API key from: https://openrouter.ai/keys
# ====================================================================

OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=openai/gpt=3.5=turbo

# Popular model alternatives:
# = google/gemini=pro (Google's Gemini)
# = anthropic/claude=2 (Anthropic Claude)
# = meta=llama/llama=2=70b=chat (Meta Llama 2)
# = mistralai/mistral=7b=instruct (Mistral)
# = gryphe/mythomax=l2=13b (Free model)
# Full list: https://openrouter.ai/models

# OpenRouter Model Parameters
# =========================================

OPENROUTER_TEMPERATURE=0.7
OPENROUTER_MAX_TOKENS=2048
OPENROUTER_TOP_P=0.9
OPENROUTER_SITE_URL=http://localhost:3000
OPENROUTER_APP_NAME=NestJS AI SaaS Starter

# =============================================================================
# OLLAMA (Alternative Local LLM Provider)
# =============================================================================
# Use Ollama for completely local LLM inference
# Make sure Ollama is installed and running: https://ollama.ai/
# Pull a model first: ollama pull llama2
# =================================================

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Alternative models: mistral, codellama, neural=chat, starling=lm
# Ollama Model Parameters
# ===================================

OLLAMA_TEMPERATURE=0.7
OLLAMA_NUM_PREDICT=256
OLLAMA_TOP_K=40
OLLAMA_TOP_P=0.9

# =============================================================================
# LOCAL EMBEDDINGS = HUGGINGFACE (Default Provider)
# =============================================================================
# Using HuggingFace models for local embeddings (no API key required for public models)

HUGGINGFACE_MODEL=sentence=transformers/all=MiniLM=L6=v2

# Alternatives:
# = sentence=transformers/all=mpnet=base=v2 (higher quality, slower)
# = sentence=transformers/paraphrase=MiniLM=L6=v2 (good for paraphrasing)
# = sentence=transformers/multi=qa=MiniLM=L6=cos=v1 (optimized for Q&A)

HUGGINGFACE_API_KEY=  # Leave empty for public models
HUGGINGFACE_BATCH_SIZE=50

# =============================================================================
# SUPERVISOR AGENT MODULE CONFIGURATION
# =============================================================================
# These settings configure the supervisor agent demo that showcases the full
# power of our NestJS-LangGraph infrastructure with all 7 child modules

# ===== Checkpoint Module =====
CHECKPOINT_ENABLED=true            # Enable checkpointing
CHECKPOINT_STORAGE=memory          # Options: memory, sqlite, redis, postgresql
CHECKPOINT_MAX_COUNT=100           # Max checkpoints to keep
CHECKPOINT_INTERVAL_MS=1000        # Checkpoint interval in milliseconds

# Redis configuration (if using redis storage)
REDIS_HOST=localhost
REDIS_PORT=6379

# PostgreSQL checkpoint configuration (if using postgresql storage)
# CHECKPOINT_DB_NAME=workflow_checkpoints
# CHECKPOINT_DB_USER=postgres
# CHECKPOINT_DB_PASSWORD=your_password

# SQLite configuration (if using sqlite storage)
# CHECKPOINT_SQLITE_PATH=./data/checkpoints.db

# ===== Memory Module =====
MEMORY_ENABLED=true                           # Enable memory module
MEMORY_CHROMADB_COLLECTION=supervisor_memory  # ChromaDB collection name (within existing ChromaDB instance)
# Note: Memory module uses the same ChromaDB/Neo4j instances as main app (configured above)
# It creates separate collections/nodes but shares the database connections

# ===== Multi-Agent Module =====
MULTI_AGENT_ENABLED=true                      # Enable multi-agent coordination
MULTI_AGENT_LIST=supervisor,researcher,analyzer # Comma-separated list of agents
MULTI_AGENT_STRATEGY=supervisor               # Options: supervisor, swarm, hierarchical
MULTI_AGENT_MAX_CONCURRENT=3                  # Max concurrent agents

# ===== Monitoring Module =====
MONITORING_ENABLED=true                       # Enable monitoring
MONITORING_WORKFLOW=true                      # Monitor workflow metrics
MONITORING_AGENTS=true                        # Monitor agent metrics
MONITORING_PERFORMANCE=true                   # Monitor performance metrics
MONITORING_ALERTING_ENABLED=true              # Enable alerting
MONITORING_ERROR_RATE_THRESHOLD=0.05          # Error rate threshold (0-1)
MONITORING_LATENCY_THRESHOLD_MS=5000          # Latency threshold in ms

# ===== Time Travel Module =====
TIME_TRAVEL_ENABLED=true                      # Enable time travel debugging
TIME_TRAVEL_MAX_SNAPSHOTS=50                  # Max snapshots to keep
TIME_TRAVEL_DEBUG_MODE=false                  # Enable debug mode

# ===== Streaming Module =====
STREAMING_ENABLED=true                        # Enable streaming
STREAMING_TOKENS=true                         # Enable token streaming
STREAMING_EVENTS=true                         # Enable event streaming
STREAMING_PROGRESS=true                       # Enable progress streaming

# WebSocket Configuration
WEBSOCKET_ENABLED=true                        # Enable WebSocket
WEBSOCKET_PORT=3001                           # WebSocket port
WEBSOCKET_CORS=true                           # Enable CORS for WebSocket

# ===== HITL (Human-in-the-Loop) Module =====
HITL_ENABLED=true                             # Enable HITL
HITL_CONFIDENCE_THRESHOLD=0.7                 # Confidence threshold (0-1)
HITL_RISK_LOW=0.3                            # Low risk threshold
HITL_RISK_MEDIUM=0.6                         # Medium risk threshold
HITL_RISK_HIGH=0.8                           # High risk threshold
HITL_RISK_CRITICAL=0.95                      # Critical risk threshold
HITL_TIMEOUT_MS=1800000                      # HITL timeout (30 minutes)

# ===== Tool System =====
TOOLS_AUTO_DISCOVER=true                      # Auto-discover tools
TOOLS_VALIDATION=true                         # Validate tools
TOOLS_CACHE=true                              # Cache tools

# ===== Compilation =====
COMPILATION_CACHE=true                        # Enable compilation cache
COMPILATION_EAGER=false                       # Enable eager compilation
COMPILATION_MAX_CACHE_SIZE=50                 # Max cache size

# ===== Debug Configuration =====
NODE_ENV=development                          # Environment: development, production
DEBUG_ENABLED=true                            # Enable debug mode
DEBUG_LOG_LEVEL=info                          # Options: debug, info, warn, error

# ===== LangGraph Platform Integration (Optional) =====
# LANGGRAPH_API_KEY=your_langgraph_key
# LANGGRAPH_ENDPOINT=your_endpoint
# WEBHOOK_SECRET=your_webhook_secret

# =============================================================================
# QUICK START
# =============================================================================
# Option A: Using OpenRouter (Default = Recommended)
# 1. Get API key from: https://openrouter.ai/keys
# 2. Set OPENROUTER_API_KEY above
# 3. Copy this file to .env
# 4. Run: npm run dev:services (starts Neo4j, ChromaDB)
# 5. Run: npx nx serve nestjs=ai=saas=starter=demo
#
# Option B: Using Ollama (Fully Local)
# 1. Install Ollama: https://ollama.ai/
# 2. Pull a model: ollama pull llama2
# 3. Start Ollama: ollama serve
# 4. Set LLM_PROVIDER=ollama
# 5. Copy this file to .env
# 6. Run: npm run dev:services (starts Neo4j, ChromaDB)
# 7. Run: npx nx serve nestjs=ai=saas=starter=demo
# =============================================================================
